---
title: "NZ otolith DTW analysis"
author: "Jens Hegg"
date: "08/15/2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r required libraries, include=FALSE}
library(readbulk)
library(dtwclust)
library(ggplot2)
library(changepoint)
library(reshape2)
library(tidyr)
library(plotly)
library(knitr)
library(mclust)
library(plyr)
library(rockchalk)
library(scales)
library(zoo)
library(psych)
library(doParallel)
library(bigmemory)
library(ggdendro)
library(cluster)
library(forcats)
library(dendextend)
library(factoextra)
library(ggpubr)
library(gdata)
library(forecast)
library(GGally)
library(readxl)
library(purrr)
library(dplyr)
library(RColorBrewer)



```

```{r reading in data, include=FALSE}

#First thing to do is to read in all the otolith data and combine it into a single long file.

#No background data

#Read in Excel file as all sheets into one dataframe (uses purrr)
#This file includes updated Otata Island archeological layers as of Oct 19, 2023

file <- '/Users/hegg1432/Dropbox/NZ Otolith DTW Analysis/Raw data files from Armagan/New Data 2023/DTW Mastersheet 2023 Updated JH Otata Corrected Oct27 2023.xlsx'

sheets <- excel_sheets(file)

raw_data_NZ  <- map_df(sheets, ~ read_excel(file, na = "NA", sheet = .x))

#Change otolith column name to fit scripts
colnames(raw_data_NZ)[17] = "OtoID"

```

```{r LOD and other background data}
#There is no provided LOD or other background data
```

```{r calculating microns from core, echo=FALSE}

#all elapsed time needs to be made into microns from the core
# Scans were 10 microns/sec

raw_data_NZ$microns = raw_data_NZ$ElapsedTime_s * 10


```

```{r investigating clipping of otolith data}

mean_CaCPS = mean(raw_data_NZ$Ca43_CPS)

Ca_CPS_hist = gghistogram(raw_data_NZ$Ca43_CPS) +
  geom_vline(xintercept = mean_CaCPS, color = "red") + 
  theme_bw()

Ca_CPS_hist

#distribution is strangely bimodal. Not sure what the deal is there. But, there are some low values that indicate poor clipping of the data. Easiest is to decide on a cutoff value on the trailing edge of the mean distribution. Most involved is to plot each one and decide on a cutoff for each otolith. Going to try the easy way and see how it goes. 

CPS_cutoff_value = 15000

Ca_CPS_hist_cutoff = Ca_CPS_hist + 
  geom_vline(xintercept = CPS_cutoff_value, color = "blue") +
  coord_cartesian(xlim = c(0, mean_CaCPS), ylim = c(0, 2500)) +
  theme_bw()


Ca_CPS_hist_cutoff
```

```{r removing low Ca CPS data}

trimmed_data_NZ = subset(raw_data_NZ, raw_data_NZ$Ca43_CPS > CPS_cutoff_value)

trimmed_Ca_CPS_hist = gghistogram(trimmed_data_NZ$Ca43_CPS)

trimmed_Ca_CPS_hist +
  theme_bw()
```


```{r summarizing otolith names and locations for later use}

OtoID_Location_NZ = trimmed_data_NZ[, c("OtoID","Location", "Time_Period", "Occupation_layer", "Occupation_layer_text")]

OtoID_Location_NZ = OtoID_Location_NZ %>% distinct()

```

```{r Finding max time for each otolith on trimmed data}
#finding the maximum for each otolith so that we can re-create microns after data is smoothed. 

#finding max microns for each otolith 
maxes_NZ = by(trimmed_data_NZ, trimmed_data_NZ$OtoID, function(x) max(x$microns))
maxes_NZ = do.call(rbind, as.list(maxes_NZ))
maxes_names__NZ = as.data.frame(row.names(maxes_NZ))
maxes_NZ = cbind(maxes_NZ, maxes_names__NZ)
colnames(maxes_NZ) = c("max_microns", "OtoID")
```

```{r looking at distribution of length for modern otoliths for clipping}
#We discussed the problem of modern otoliths potentially having shorter lifespans, biasing DTW becuase the older otoliths simply have more pattern due to more time alive. So, figuring out where we should clip otoliths. We discussed ~3000 microns in our meeting. 

time_periods = trimmed_data_NZ[, c("OtoID", "Time_Period")]

time_periods = unique(time_periods)

maxes_NZ = merge(maxes_NZ, time_periods, by = "OtoID")

length_hist = ggplot(maxes_NZ, aes(x=max_microns))

length_hist +
  geom_histogram(aes(color = Time_Period)) +
  facet_wrap(.~Time_Period)

maxes_NZ %>% 
  group_by(Time_Period) %>% 
  summarise(length = mean(max_microns), Median = median(max_microns), StdDev = sd(max_microns))
```
```{r limiting length to 3000microns}

#cutting off all the transects to 3000 microns 
trimmed_data_NZ = subset(trimmed_data_NZ, microns<=3000)

max(trimmed_data_NZ$microns)

```
```{r Finding max time again for each otolith on data limited to 3000microns}
#finding the maximum for each otolith so that we can re-create microns after data is smoothed. 

#finding max microns for each otolith 
maxes_NZ = by(trimmed_data_NZ, trimmed_data_NZ$OtoID, function(x) max(x$microns))
maxes_NZ = do.call(rbind, as.list(maxes_NZ))
maxes_names__NZ = as.data.frame(row.names(maxes_NZ))
maxes_NZ = cbind(maxes_NZ, maxes_names__NZ)
colnames(maxes_NZ) = c("max_microns", "OtoID")
```


``` {r creating list of data for DTW}
#------------------------------------------------------------
# Making list of multivariate transects for DTW
#------------------------------------------------------------

#split into list by OtoID column
trimmed_data_NZ_list = split(trimmed_data_NZ[,4:16], trimmed_data_NZ$OtoID)

#removing outlier data with three rounds of tsclean() from {forecast}, then smoothing using a rolling average since the plots are really difficult to interpret due to noise

#Doubtless/Harukai outlier detection and smoothing
trimmed_data_NZ_smooth = lapply(trimmed_data_NZ_list, function(df){
  
    #running tsclean() three times to remove outliers
    ts <- ts(df)
      
      for (i in 1:ncol(ts)) {
        df[, i] <- tsclean(ts[, i])
      }
    
      for (i in 1:ncol(ts)) {
          df[, i] <- tsclean(ts[, i])
      }
    
      for (i in 1:ncol(ts)) {
        df[, i] <- tsclean(ts[, i])
      }
      
      #Applying a strong rolling average to deal with noise
      x = rollapply(df, width=100, FUN=mean, by.column=TRUE, fill=c(NA, 0, NA), align="right")
  
    return(as.data.frame(na.omit(x)))
    
  }
)

#average length of otoliths 
length_data_NZ = lapply(trimmed_data_NZ_smooth, dim)

av_length_data_NZ = colMeans(do.call(rbind,length_data_NZ))
av_length_data_NZ

#reinterpolate to the mean length
data_NZ_list_reinterp = lapply(trimmed_data_NZ_smooth, function(x) reinterpolate(t(as.matrix(x)), 1392L))

#turn all the matrices back over. why this function deals with everything in different orientations is mystifying
data_NZ_list_reinterp = lapply(data_NZ_list_reinterp, FUN=t)

#dtwclust freaks out unless the data is in matrix but having it as a dataframe is easier to deal with
data_NZ_list_reinterp_df = lapply(data_NZ_list_reinterp, FUN=as.data.frame)
```

```{r creating dataframe from smoothed data}
#Creating dataframe from list of smoothed/outlier corrected lists for use in plotting later

trimmed_data_NZ_smooth_df = bind_rows(trimmed_data_NZ_smooth, .id = "OtoID")

#Add max column and recreate microns for smoothed data
trimmed_data_NZ_smooth_df$maxes = maxes_NZ$max_microns[match(trimmed_data_NZ_smooth_df$OtoID, maxes_NZ$OtoID)]

#Add sequence column to calculate microns from max
trimmed_data_NZ_smooth_df$seq <- with(trimmed_data_NZ_smooth_df, ave(seq_along(OtoID), OtoID, FUN=seq_along))

#max seq of each otolith
#finding max microns for each otolith 
max_seq_NZ = by(trimmed_data_NZ_smooth_df, trimmed_data_NZ_smooth_df$OtoID, function(x) max(x$seq))
max_seq_NZ = do.call(rbind, as.list(max_seq_NZ))
max_seq_names_NZ = as.data.frame(row.names(max_seq_NZ))
max_seq_NZ = cbind(max_seq_NZ, max_seq_names_NZ)
colnames(max_seq_NZ) = c("max_seq", "OtoID")

trimmed_data_NZ_smooth_df$max_seq = max_seq_NZ$max_seq[match(trimmed_data_NZ_smooth_df$OtoID, max_seq_NZ$OtoID)]

trimmed_data_NZ_smooth_df$microns = (trimmed_data_NZ_smooth_df$maxes/trimmed_data_NZ_smooth_df$max_seq)*trimmed_data_NZ_smooth_df$seq
  
trimmed_data_NZ_smooth_df = merge(trimmed_data_NZ_smooth_df, OtoID_Location_NZ, by = "OtoID")
```


```{r calculating descriptive statistics to cluster}
#------------------------------------------------------------
# Making dataframe of descriptive statistics (mean, sd, skew) to do a primary clustering on in case mean makes a difference
#------------------------------------------------------------

#creates discriptive statistics on the reinterpolated data. Could be done on raw data if we thought length was not a factor on mean

descr_stats_combined = 
aggregate(trimmed_data_NZ_smooth_df[,2:14], list(trimmed_data_NZ_smooth_df$OtoID), mean)

colnames(descr_stats_combined)[1] = "OtoID"


#merging location/time info by OtoID
descr_stats_combined = merge(descr_stats_combined, OtoID_Location_NZ, by = "OtoID")
```

```{r Clustering on mean first}
#scaling data
sc_descr_stats_combined = scale(descr_stats_combined[2:14], center = TRUE, scale = TRUE)

sc_descr_stats_combined = cbind(descr_stats_combined[c(1, 15)], as.data.frame(sc_descr_stats_combined))

#using mClust to cluster on all columns
mclust_combined = Mclust(as.data.frame(sc_descr_stats_combined[,3:15]))

#The groupings are hard to decipher. Not super clear cut
#plot(mclust_combined)

#adding classifications to dataframe
sc_descr_stats_combined$mclust_class = mclust_combined$classification
```

```{r plotting labelled by known category}

combined_test_plot = ggplot(sc_descr_stats_combined, aes(x=Sr_ug_g_m88, y=Na_ug_g_m23, color = Location))+
  geom_point() +
  theme_bw()

combined_test_plot

#Pairs plot (requires GGally package)

combined_pairs_plot = ggpairs(sc_descr_stats_combined, columns = 3:15, aes(color = as.factor(mclust_class)))

combined_pairs_plot

#pairs plot shows Na, K, P as the main sources of separation by mean. This is fascinating, is it a metabolic difference? Would be interesting to map to location/time later on. But, because it's metabolic I think DTW makes sense. 
```

```{r DTW clustering of time series}
#Timing the process
#Starting the clock
ptm <- proc.time()

# #Subsetting to elements I know to try to get some meaningful plots
# Doubtless_Harukai_list_reinterp_coreElements = lapply(combined_data_list_smooth_reinterp, function(x) x[,c(3,7,11,12)])

##Running the DTW cluster analysis

#running with 8 parallel processors since I think this will be a long calculation
require("doParallel")
# Create parallel workers
workers <- makeCluster(8L)
# Preload dtwclust in each worker; not necessary but useful
invisible(clusterEvalQ(workers, library("dtwclust")))
# Register the backend; this step MUST be done
registerDoParallel(workers)

#running the actual clustering
combined_dtw_heir_clust_MV = tsclust(series = data_NZ_list_reinterp, 
                               preproc = zscore, 
                               k = 2,
                               type = "hierarchical", 
                               distance="DTW", 
                               window.size=100, 
                               control=hierarchical_control(method="ward.D"), 
                               trace=TRUE)

# Stop parallel workers
stopCluster(workers)
# Go back to sequential computation
registerDoSEQ()

# Stoping the clock
proc.time() - ptm

##last time it was run on Doubtless_Harukai
#   user  system elapsed 
#  3.731   5.692 867.317 - 14minutes!!

# Last time this was run on combined dataset
#Elapsed time is 2023.248 seconds - 34 minutes!
# 
#     user   system  elapsed 
#    6.615    9.810 2030.991 
#
#Run on combined data for 2023
#Elapsed time is 1489.55 seconds.
# 
#     user   system  elapsed 
#    5.432    5.132 1503.553 - 33 minutes! Not bad for quite a bit more data

# Elapsed time is 1192.192 seconds. (08/16/2023)
# 
#     user   system  elapsed 
#    4.328    4.273 1208.754 - 12 minutes. Wow! this ran alot faster
# 
# proc.time() - ptm
#     user   system  elapsed  - 20 minutes 10/21/2023
#    4.263    4.152 1203.054 
```

```{r detailed dendrograms of all data}

# dendrogram just to see what it looks like
fviz_dend(combined_dtw_heir_clust_MV,               
          k=5, # Cut in k groups
          ylim = c(0, 200000),
          cex = 0.5, 
          show_labels = FALSE, 
          color_labels_by_k = FALSE, 
          k_colors = "jco",
          rect = TRUE, 
          rect_border = "jco", 
          rect_fill = TRUE, 
          #color_labels_by_k = TRUE,                 
          horiz = FALSE,
          ggtheme = theme_void()                       
          )

#looking at this dendrogram 8 clusters looks logical
combined_cuts_MV = as.data.frame(cutree(combined_dtw_heir_clust_MV,
                                  k=8,
                                  order_clusters_as_data = FALSE))
colnames(combined_cuts_MV) = "Hier_Clust_5"
combined_cuts_MV$OtoID = rownames(combined_cuts_MV)

#But, there are a few ways to see this and none of them have great sillouette values. Lets be able to look at multiple groups easily

combined_cuts_MV_2 = as.data.frame(rownames(combined_cuts_MV))
colnames(combined_cuts_MV_2) = "OtoID"

for (i in 2:8){
  x = as.data.frame(cutree(combined_dtw_heir_clust_MV,
                                  k=i,
                                  order_clusters_as_data = FALSE))
  colnames(x) = paste("Hier_Clust_", i, sep = "")
  combined_cuts_MV_2 = cbind(combined_cuts_MV_2, x)
}

## Silhouette for hierarchical clusters
#checking cluster decision
sillouette_DH_MV = cvi(combined_dtw_heir_clust_MV, type = "valid")


#merge with un-interpolated but smoothed/outlier removed time-series and plot 
combined_DTW_transects = merge(combined_cuts_MV_2, trimmed_data_NZ_smooth_df, by = "OtoID")

##melt the data to plot
combined_DTW_melt_for_plot_MV=melt(combined_DTW_transects, id.vars=c("OtoID", "Hier_Clust_2" , "Hier_Clust_3", "Hier_Clust_4", "Hier_Clust_5", "Hier_Clust_6", "Hier_Clust_7", "Hier_Clust_8", "maxes", "seq", "max_seq", "microns", "Location", "Time_Period", "Occupation_layer", "Occupation_layer_text"))

combined_DTW_melt_for_plot_MV = combined_DTW_melt_for_plot_MV %>% 
  rename(Element = variable, Ratio = value)

combined_DTW_melt_for_plot_MV$Element_short = sub("\\_.*", "", combined_DTW_melt_for_plot_MV$Element)

#Sorting dataframe
combined_DTW_melt_for_plot_MV = combined_DTW_melt_for_plot_MV[order(combined_DTW_melt_for_plot_MV$OtoID, combined_DTW_melt_for_plot_MV$microns),]

#Making the Time_Period column order correctly
combined_DTW_melt_for_plot_MV$Time_Period = factor(combined_DTW_melt_for_plot_MV$Time_Period, levels = c("14th Century", "15th Century","17-18th Century", "20th Century", "Present day"))

#Unique elements analyzed
unique_elements = as.vector(unique(combined_DTW_melt_for_plot_MV$Element_short))

##create pdf where each page is a separate plot.
#prints until dev.off is called

pdf("Transects_TimePeriod_by_8clusts.pdf", width = 11, height = 8.5)

for (i in 1:length(unique_elements)) {

#creating facet plot
print(
  ggplot(data=combined_DTW_melt_for_plot_MV[which(combined_DTW_melt_for_plot_MV$Element_short == unique_elements[i]),], 
         aes(x=microns, y=Ratio)) +
  geom_line(aes(color = Location, group = OtoID), linewidth=0.2) +
  facet_grid(Hier_Clust_8~Time_Period) +
  guides(x = guide_axis(n.dodge = 2)) +
  scale_color_manual(values = c("#d01e89", "#9bd2a4", "#ff7f00", "#984ea3", "#e41a1c", "#377eb8", "#4daf4a", "#cab2d6")) +
  theme_bw()+
  guides(color = guide_legend(override.aes = list(size=5))) +
  labs(x="Microns from Otolith Core", y = paste(unique_elements[i], "(ug/g)"), title = paste(unique_elements[i], "by Group & Time Period"))
)

}
dev.off()

#percentage bar plots of cluster proportions within age groupings

#add cluster data to summarized means
means_data = merge(descr_stats_combined, combined_cuts_MV_2, by = "OtoID")

for (i in 19:25){

grp_num = length(unique(means_data[,i]))
pdf_name = paste("Grouping Percentage Plot -", grp_num, "Clusters.pdf")
palette_num = brewer.pal(name = "RdYlGn", n = grp_num)
  
pdf(pdf_name, width = 11, height = 8.5)

agegroup_percent_plot_MV = ggplot(means_data, aes(x = Time_Period))

print(
agegroup_percent_plot_MV +
  geom_bar(aes(fill = as.factor(means_data[,i])), position = "fill") +
  scale_fill_manual(values = palette_num, name="Cluster") + 
  scale_y_continuous(labels = scales::percent) +
  labs(x="Time Period Group", y = "Cluster %", title = paste("Makeup of Each Time Period by Cluster -", grp_num, "Clusters")) +
  theme_bw()
)

dev.off()
}

#plot of means for each time period by element
descr_stats_combined_melted = melt(descr_stats_combined, id.vars = c("OtoID", "Location", "Time_Period", "Occupation_layer", "Occupation_layer_text"))

descr_stats_combined_melted$Time_Period = factor(descr_stats_combined_melted$Time_Period, levels = c("14th Century", "15th Century", "Late 14th to Mid-16th Century", "Mid 17th to mid-19th Century", "20th Century", "Present day"))

descr_stats_combined_melted = descr_stats_combined_melted %>% 
  rename(Element = variable, Ratio = value)

descr_stats_combined_melted = descr_stats_combined_melted %>% 
  mutate(Modern_Hist = if_else(Time_Period == c("Present Day", "20th Century"), "Modern", "Pre-20th Century"))

descr_stats_combined_melted$Modern_Hist = factor(descr_stats_combined_melted$Modern_Hist, levels = c("Pre-20th Century", "Modern"))

descr_stats_combined_melted$Element_short = sub("\\_.*", "", descr_stats_combined_melted$Element)


element_means_by_time_plot = ggplot(descr_stats_combined_melted, aes(x=Time_Period, y=Ratio))

element_means_by_time_plot +
  geom_boxplot() +
  facet_grid(Element_short~., scales = "free_y") +
  labs(title = "Mean of Entire Otolith")

element_means_by_ModHist_plot = ggplot(subset(descr_stats_combined_melted, Element_short == c("P", "Al", "Cu", "Zn", "Li")), aes(x=Modern_Hist, y=Ratio))

element_means_by_ModHist_plot +
  geom_boxplot() +
  facet_grid(Element_short~., scales = "free_y") +
  labs(title = "Mean of Entire Otolith")

#table of cluster proportions
# prop_cluster_data_DTW_DH_MV = DH_DTW_melt_for_plot_MV[,c( "File", "Hier_Clust_4")]
# 
# cluster_n_DTW_DH_MV = table(droplevels(prop_cluster_data_DTW_DH_MV))
# 
# cluster_prop_DTW_DH_MV = round(100*prop.table(cluster_n_DTW_DH_MV, margin = 2), digits = 1)
# 
# cluster_n_DTW_DH_MV
# cluster_prop_DTW_DH_MV
# 
# cbind(cluster_n_DTW_DH_MV, cluster_prop_DTW_DH_MV)
```

```{r calculating descriptive statistics for just juvenile portion}
#------------------------------------------------------------
# Making dataframe of descriptive statistics (mean, sd, skew) to do a primary clustering on in case mean makes a difference
#------------------------------------------------------------

#creates discriptive statistics on the reinterpolated data. Could be done on raw data if we thought length was not a factor on mean

descr_stats_juvy_1 = subset(trimmed_data_NZ_smooth_df, microns <=1000)
descr_stats_juvy = aggregate(descr_stats_juvy_1[,2:14], list(descr_stats_juvy$OtoID), mean)

colnames(descr_stats_juvy)[1] = "OtoID"


#merging location/time info by OtoID
descr_stats_juvy = merge(descr_stats_juvy, OtoID_Location_NZ, by = "OtoID")

descr_stats_juvy_melted = melt(descr_stats_juvy, id.vars = c("OtoID", "Location", "Time_Period", "Occupation_layer", "Occupation_layer_text"))

descr_stats_juvy_melted$Time_Period = factor(descr_stats_juvy_melted$Time_Period, levels = c("14th Century", "15th Century", "Late 14th to Mid-16th Century", "Mid 17th to mid-19th Century", "20th Century", "Present day"))

descr_stats_juvy_melted = descr_stats_juvy_melted %>% 
  rename(Element = variable, Ratio = value)

descr_stats_juvy_melted = descr_stats_juvy_melted %>% 
  mutate(Modern_Hist = if_else(Time_Period == c("Present Day", "20th Century"), "Modern", "Pre-20th Century"))

descr_stats_juvy_melted$Modern_Hist = factor(descr_stats_juvy_melted$Modern_Hist, levels = c("Pre-20th Century", "Modern"))

descr_stats_juvy_melted$Element_short = sub("\\_.*", "", descr_stats_juvy_melted$Element)

#plots
element_juvy_means_by_time_plot = ggplot(descr_stats_juvy_melted, aes(x=Time_Period, y=Ratio))

element_juvy_means_by_time_plot +
  geom_boxplot() +
  facet_grid(Element_short~., scales = "free_y") +
  labs(title = "Juvenile Period (<1000microns)")

element_juvy_means_by_ModHist_plot = ggplot(subset(descr_stats_juvy_melted, Element_short == c("P", "Al", "Cu", "Zn", "Li")), aes(x=Modern_Hist, y=Ratio))

element_means_by_ModHist_plot +
  geom_boxplot() +
  facet_grid(Element_short~., scales = "free_y") +
  labs(title = "Juvenile Period (<1000microns)")
```

```{r plotting layers data}

#datasest with only the otoliths that have layer info
layer_data = subset(combined_DTW_melt_for_plot_MV, (!is.na(combined_DTW_melt_for_plot_MV[,12])))


##create pdf where each page is a separate plot.
#prints until dev.off is called
pdf("Transect_Layers_by_TimePeriod_plots2.pdf", width = 11, height = 8.5)

for (i in 1:length(unique_elements)) {

#creating facet plot
print(ggplot(data=layer_data[ which(layer_data$Element_short == unique_elements[9]),], aes(x=microns, y=Ratio))+
  geom_line(aes(color = as.factor(Occupation_layer), group = OtoID), linewidth=0.2) +
  facet_grid(Time_Period + Location~Hier_Clust_5) +
  guides(x = guide_axis(n.dodge = 2)) +
  #coord_cartesian(ylim=c(.705, .7155), xlim = c(0, 1000)) + 
  #geom_hline(yintercept=0.70918, color="blue")+
  scale_color_manual(values = c("#d01e89", "#9bd2a4", "#ff7f00", "#984ea3", "#e41a1c", "#377eb8", "#4daf4a", "#cab2d6"), 
                     name="Occupation\nLayer") +
  theme_bw()+
  guides(color = guide_legend(override.aes = list(size=5))) +
  labs(x="Microns from Otolith Core", y = paste(unique_elements[9], "(ug/g)"), title = paste(unique_elements[9], "by Group & Time Period"))# + #+
  #theme(aspect.ratio = 1) #+
  #coord_cartesian(ylim = c(0, 12000))
)

}
dev.off()

#percentage bar plot of layer data

layer_count_data = unique(layer_data[,c(1:3, 11:12)])

layer_percent_plot_MV = ggplot(layer_count_data, aes(x = as.factor(Hier_Clust_5)))

layer_percent_plot_MV +
  geom_bar(aes(fill = as.factor(Occupation_layer)), position = "fill") +
  facet_grid(Time_Period+Location~.) +
  scale_fill_manual(values = Dark2, name="Occupation\nLayer") +
  scale_y_continuous(labels = scales::percent) +
  labs(x="Clustering Group", y = "Occupation Layer %", title = "Makeup of Each Cluster Group by Occupation Layer\nLowest = Earliest") +
  theme_bw()
  #guides(color = guide_legend(override.aes = list(size=5)))

#table of cluster proportions
prop_cluster_data_DTW_DH_MV = DH_DTW_melt_for_plot_MV[,c( "File", "Hier_Clust_4")]

cluster_n_DTW_DH_MV = table(droplevels(prop_cluster_data_DTW_DH_MV))

cluster_prop_DTW_DH_MV = round(100*prop.table(cluster_n_DTW_DH_MV, margin = 2), digits = 1)

cluster_n_DTW_DH_MV
cluster_prop_DTW_DH_MV

cbind(cluster_n_DTW_DH_MV, cluster_prop_DTW_DH_MV)

```
